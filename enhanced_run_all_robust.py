# enhanced_run_all_robust.py - Improved version with better file naming for comparison
import json
import traceback
import os
from pathlib import Path
import pandas as pd
import time

# Configuration
repo_root = Path(__file__).resolve().parent
mock_dir = repo_root / "data" / "mock_files"
out_dir = repo_root / "outputs"
enhanced_out_dir = repo_root / "outputs_enhanced_robust"

# Ensure directories exist
out_dir.mkdir(parents=True, exist_ok=True)
enhanced_out_dir.mkdir(parents=True, exist_ok=True)

print(f"üìÅ Input directory: {mock_dir}")
print(f"üìÅ Original output directory: {out_dir}")
print(f"üìÅ Enhanced output directory: {enhanced_out_dir}")

# Import existing extractors with fallbacks
try:
    from src.extract_pdf import extract_from_pdf
except Exception:
    def extract_from_pdf(path):
        return {'error': 'extract_from_pdf not available'}

try:
    from src.extract_ocr import extract_from_image
except Exception:
    def extract_from_image(path):
        return {'error': 'extract_from_image not available'}

try:
    from src.extract_excel import extract_excel as _extract_excel
    def extract_from_excel(path):
        return _extract_excel(path)
except Exception:
    def extract_from_excel(path):
        try:
            import pandas as pd
            df = pd.read_excel(path, sheet_name=0)
            items = []
            try:
                items_df = pd.read_excel(path, sheet_name='Items')
                items = items_df.to_dict(orient='records')
            except Exception:
                items = df.to_dict(orient='records') if not df.empty else []
            return {
                'order_id': None, 'client_name': None, 'order_date': None,
                'delivery_date': None, 'items': items,
                'order_total': None, 'currency': None, 'special_instructions': None,
                'confidence_score': 0.7
            }
        except Exception as e:
            return {'error': f'excel extraction failed: {e}'}

def extract_from_csv(path):
    try:
        import pandas as pd
        df = pd.read_csv(path)
        return {
            'order_id': None, 'client_name': None, 'order_date': None,
            'delivery_date': None, 'items': df.to_dict(orient='records'),
            'order_total': None, 'currency': None, 'special_instructions': None,
            'confidence_score': 0.6
        }
    except Exception as e:
        return {'error': str(e)}

# Standard handlers
handlers = {
    '.pdf': lambda p: extract_from_pdf(str(p)),
    '.png': lambda p: extract_from_image(str(p)),
    '.jpg': lambda p: extract_from_image(str(p)),
    '.jpeg': lambda p: extract_from_image(str(p)),
    '.tiff': lambda p: extract_from_image(str(p)),
    '.bmp': lambda p: extract_from_image(str(p)),
    '.xlsx': lambda p: extract_from_excel(str(p)),
    '.xls': lambda p: extract_from_excel(str(p)),
    '.csv': lambda p: extract_from_csv(str(p)),
}

# UPDATE the handlers dictionary to use the simple version:
handlers['.txt'] = lambda p: extract_from_txt_simple(str(p))
handlers['.text'] = lambda p: extract_from_txt_simple(str(p))

print("üîß FOCUSED FIX LOADED - .txt files should now be processed!")

# Robust LLM Enhancement Class
class SimpleLLMEnhancer:
    def __init__(self, api_key):
        from groq import Groq
        self.client = Groq(api_key=api_key)
        self.model = "Llama-3.3-70B-Versatile"

            
    def safe_llm_call(self, prompt, max_retries=2):
        """Make LLM call with retries and error handling"""
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    max_tokens=1500,
                    timeout=20
                )
                
                content = response.choices[0].message.content
                if content and content.strip():
                    return content.strip()
                    
            except Exception as e:
                print(f"      LLM call attempt {attempt + 1} failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
        
        return None
    
    def safe_json_parse(self, response_text, fallback=None):
        """Safely parse JSON response with fallback"""
        if not response_text:
            return fallback or {}
        
        # Try direct JSON parsing
        try:
            return json.loads(response_text)
        except json.JSONDecodeError:
            pass
        
        # Try to extract JSON from text
        import re
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass
        
        # Manual extraction as last resort
        result = fallback or {}
        
        # Extract key patterns
        patterns = {
            'order_id': r'order[_\s]*id[:\s]*([A-Z0-9\-_]+)',
            'client_name': r'client[_\s]*name[:\s]*([^,\n]+)',
            'confidence_score': r'confidence[:\s]*(\d+\.?\d*)',
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, response_text, re.IGNORECASE)
            if match:
                value = match.group(1).strip()
                if key == 'confidence_score':
                    try:
                        result[key] = float(value)
                    except ValueError:
                        pass
                else:
                    result[key] = value
        
        return result
    
    def enhance_metadata_simple(self, raw_content, current_result):
        """Simple metadata enhancement with robust error handling"""
        
        # Limit content size
        content_sample = raw_content[:1500] if len(raw_content) > 1500 else raw_content
        
        prompt = f"""
        Extract purchase order information from this document. 
        Respond with ONLY a JSON object, no other text:
        
        Document: {content_sample}
        
        {{
            "order_id": "found_order_id_or_null",
            "client_name": "found_client_name_or_null",
            "order_date": "YYYY-MM-DD_or_null",
            "delivery_date": "YYYY-MM-DD_or_null", 
            "order_total": 123.45,
            "currency": "USD",
            "confidence_score": 0.8
        }}
        """
        
        response = self.safe_llm_call(prompt)
        enhanced = self.safe_json_parse(response, current_result.copy())
        
        return enhanced
    
    def enhance_excel_simple(self, raw_content, current_result):
        """Simple Excel enhancement"""
        
        items_sample = current_result.get('items', [])[:3]  # First 3 items
        
        prompt = f"""
        Process this Excel/CSV purchase order data.
        Respond with ONLY a JSON object, no other text:
        
        Current items: {json.dumps(items_sample, default=str)}
        
        {{
            "order_id": "extracted_from_items_or_existing",
            "client_name": "inferred_client_name",
            "order_total": 123.45,
            "confidence_score": 0.85
        }}
        """
        
        response = self.safe_llm_call(prompt)
        enhanced = self.safe_json_parse(response, current_result.copy())
        
        return enhanced
    
    def enhance_scanned_simple(self, raw_content, current_result):
        """Simple scanned document enhancement"""
        
        content_sample = raw_content[:1500] if len(raw_content) > 1500 else raw_content
        
        prompt = f"""
        Extract data from this scanned purchase order.
        Respond with ONLY a JSON object, no other text:
        
        OCR Text: {content_sample}
        
        {{
            "order_id": "corrected_order_id",
            "client_name": "extracted_client",
            "items": [
                {{
                    "product_code": "code",
                    "description": "description",
                    "quantity": 1,
                    "unit_price": 10.0,
                    "total_price": 10.0
                }}
            ],
            "confidence_score": 0.75
        }}
        """
        
        response = self.safe_llm_call(prompt)
        enhanced = self.safe_json_parse(response, current_result.copy())
        
        return enhanced

# Initialize LLM enhancer
llm_enhancer = None
#openai_api_key = os.getenv('OPENAI_API_KEY')
groq_api_key = os.getenv('GROQ_API_KEY')

if groq_api_key:
    try:
        llm_enhancer = SimpleLLMEnhancer(groq_api_key)
        print("‚úì Robust LLM enhancer initialized")
    except Exception as e:
        print(f"‚ö† LLM enhancer failed to initialize: {e}")
else:
    print("‚ö† GROQ_API_KEY not set - running without LLM enhancement")

def get_raw_content(file_path):
    """Extract raw content for LLM processing"""
    try:
        path = Path(file_path)
        
        if path.suffix.lower() in ['.xlsx', '.xls']:
            import pandas as pd
            try:
                # Try to read multiple sheets
                excel_file = pd.ExcelFile(str(path))
                content_parts = []
                for sheet_name in excel_file.sheet_names[:3]:  # First 3 sheets
                    df = pd.read_excel(str(path), sheet_name=sheet_name)
                    content_parts.append(f"Sheet {sheet_name}: {df.head(10).to_string()}")
                return "\n\n".join(content_parts)
            except Exception:
                return f"Excel file: {path.name}"
        
        elif path.suffix.lower() == '.csv':
            import pandas as pd
            try:
                df = pd.read_csv(str(path))
                return df.head(10).to_string()
            except Exception:
                return f"CSV file: {path.name}"
        
        elif path.suffix.lower() in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']:
            try:
                import pytesseract
                from PIL import Image
                img = Image.open(str(path))
                return pytesseract.image_to_string(img)
            except Exception:
                return f"Image file: {path.name}"
        
        elif path.suffix.lower() in ['.txt']:
            return path.read_text()[:2000]  # Limit size
        
        return f"File: {path.name}"
        
    except Exception as e:
        print(f"      Error extracting content from {file_path}: {e}")
        return ""

def should_enhance_robust(file_path, result):
    """Robust enhancement decision logic"""
    
    # Skip if error occurred
    if result.get('error'):
        return False
    
    # Always enhance if confidence < 0.8
    if result.get('confidence_score', 0) < 0.8:
        return True
    
    # Always enhance Excel/CSV files
    if file_path and Path(file_path).suffix.lower() in ['.xlsx', '.xls', '.csv']:
        return True
    
    # Always enhance image files
    if file_path and Path(file_path).suffix.lower() in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']:
        return True
    
    # Enhance if missing critical data
    if not result.get('order_id') or not result.get('client_name'):
        return True
    
    # Enhance if no items
    if not result.get('items'):
        return True
    
    return False

def robust_enhance_extraction(enhancer, raw_content, current_result, file_path):
    """Robust enhancement with comprehensive error handling"""
    
    result = current_result.copy()
    strategies_applied = []
    
    print(f"    üõ†Ô∏è Applying robust enhancement...")
    
    # Strategy 1: Metadata enhancement (always try)
    try:
        print(f"      üìù Enhancing metadata...")
        metadata_enhanced = enhancer.enhance_metadata_simple(raw_content, result)
        if metadata_enhanced:
            # Only update fields that were actually improved
            for key, value in metadata_enhanced.items():
                if value and (not result.get(key) or key == 'confidence_score'):
                    result[key] = value
            strategies_applied.append("metadata_enhanced")
            print(f"      ‚úì Metadata enhancement applied")
    except Exception as e:
        print(f"      ‚ö† Metadata enhancement failed: {e}")
        strategies_applied.append(f"metadata_failed: {str(e)[:30]}")
    
    # Strategy 2: File-type specific enhancement
    if file_path:
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext in ['.xlsx', '.xls', '.csv']:
            try:
                print(f"      üìä Enhancing Excel/CSV...")
                excel_enhanced = enhancer.enhance_excel_simple(raw_content, result)
                if excel_enhanced:
                    for key, value in excel_enhanced.items():
                        if value and (not result.get(key) or key == 'confidence_score'):
                            result[key] = value
                    strategies_applied.append("excel_enhanced")
                    print(f"      ‚úì Excel/CSV enhancement applied")
            except Exception as e:
                print(f"      ‚ö† Excel/CSV enhancement failed: {e}")
                strategies_applied.append(f"excel_failed: {str(e)[:30]}")
        
        elif file_ext in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']:
            try:
                print(f"      üñºÔ∏è Enhancing scanned document...")
                scanned_enhanced = enhancer.enhance_scanned_simple(raw_content, result)
                if scanned_enhanced:
                    for key, value in scanned_enhanced.items():
                        if value and (not result.get(key) or key in ['confidence_score', 'items']):
                            result[key] = value
                    strategies_applied.append("scanned_enhanced")
                    print(f"      ‚úì Scanned document enhancement applied")
            except Exception as e:
                print(f"      ‚ö† Scanned document enhancement failed: {e}")
                strategies_applied.append(f"scanned_failed: {str(e)[:30]}")
    
    # Ensure confidence score is reasonable
    if not result.get('confidence_score') or result['confidence_score'] < current_result.get('confidence_score', 0):
        result['confidence_score'] = max(current_result.get('confidence_score', 0.5), 0.6)
    
    # Add enhancement info
    result['enhancement_info'] = {
        'enhanced_by_llm': len([s for s in strategies_applied if 'enhanced' in s]) > 0,
        'enhancement_strategies': strategies_applied,
        'enhancement_timestamp': pd.Timestamp.now().isoformat(),
        'robust_mode': True,
        'original_confidence': current_result.get('confidence_score', 0),
        'final_confidence': result.get('confidence_score', 0)
    }
    
    return result

def get_baseline_filename(file_path):
    """Generate consistent baseline filename for comparison"""
    stem = file_path.stem
    return f"{stem}_baseline.json"

def get_enhanced_filename(file_path):
    """Generate consistent enhanced filename for comparison"""
    stem = file_path.stem
    return f"{stem}_enhanced_robust.json"

def ensure_baseline_exists(file_path):
    """Ensure baseline file exists for comparison"""
    baseline_path = out_dir / get_baseline_filename(file_path)
    
    if not baseline_path.exists():
        print(f"    üìã Creating baseline for comparison: {baseline_path.name}")
        
        # Run standard extraction
        ext = file_path.suffix.lower()
        handler = handlers.get(ext)
        
        if handler is None:
            baseline_result = {'error': f'unsupported file type: {ext}', 'confidence_score': 0.0}
        else:
            try:
                baseline_result = handler(file_path)
                baseline_result['processing_mode'] = 'baseline'
                baseline_result['processing_timestamp'] = pd.Timestamp.now().isoformat()
            except Exception as e:
                baseline_result = {'error': str(e), 'confidence_score': 0.0}
        
        # Save baseline
        try:
            with open(baseline_path, 'w', encoding='utf-8') as f:
                json.dump(baseline_result, f, indent=2, default=str)
            print(f"    ‚úì Baseline saved: {baseline_path.name}")
        except Exception as e:
            print(f"    ‚ùå Failed to save baseline: {e}")
    
    return baseline_path

def process_file_robust(file_path):
    """Process a single file with robust enhancement and ensure baseline exists"""
    
    print(f"\n{'='*60}")
    print(f"Processing: {file_path.name}")
    print(f"{'='*60}")
    
    # Ensure baseline exists for comparison
    baseline_path = ensure_baseline_exists(file_path)
    
    # Load baseline if it exists
    baseline_result = {}
    if baseline_path.exists():
        try:
            with open(baseline_path, 'r', encoding='utf-8') as f:
                baseline_result = json.load(f)
            print(f"  üìä Baseline loaded: {baseline_result.get('confidence_score', 0):.2f} confidence")
        except Exception as e:
            print(f"  ‚ö† Failed to load baseline: {e}")
    
    # Standard extraction for enhancement
    ext = file_path.suffix.lower()
    handler = handlers.get(ext)
    
    if handler is None:
        result = {'error': f'unsupported file type: {ext}', 'confidence_score': 0.0}
    else:
        try:
            result = handler(file_path)
        except Exception as e:
            result = {'error': str(e), 'confidence_score': 0.0}
    
    print(f"  üîß Initial: {result.get('confidence_score', 0):.2f} confidence, {len(result.get('items', []))} items")
    
    # Enhanced processing
    enhanced_result = result.copy()
    
    if llm_enhancer and should_enhance_robust(file_path, result):
        try:
            raw_content = get_raw_content(file_path)
            if raw_content.strip():
                enhanced_result = robust_enhance_extraction(
                    llm_enhancer, raw_content, result, str(file_path)
                )
                
                orig_conf = result.get('confidence_score', 0)
                enh_conf = enhanced_result.get('confidence_score', 0)
                improvement = enh_conf - orig_conf
                
                strategies = enhanced_result.get('enhancement_info', {}).get('enhancement_strategies', [])
                success_count = len([s for s in strategies if 'enhanced' in s])
                
                print(f"  üöÄ Enhanced: {enh_conf:.2f} confidence (Œî{improvement:+.2f}), {success_count} strategies applied")
            else:
                print(f"  ‚ÑπÔ∏è No content available for enhancement")
        except Exception as e:
            print(f"  ‚ùå Enhancement completely failed: {e}")
            enhanced_result['enhancement_error'] = str(e)
    else:
        reason = "not needed" if not should_enhance_robust(file_path, result) else "LLM unavailable"
        print(f"  ‚è≠Ô∏è Skipping enhancement ({reason})")
    
    # Add processing metadata with comparison info
    enhanced_result['processing_metadata'] = {
        'file_name': file_path.name,
        'file_size_bytes': file_path.stat().st_size,
        'processing_timestamp': pd.Timestamp.now().isoformat(),
        'processing_mode': 'enhanced_robust',
        'llm_enhanced': enhanced_result.get('enhancement_info', {}).get('enhanced_by_llm', False),
        'final_confidence': enhanced_result.get('confidence_score', 0.0),
        'needs_review': enhanced_result.get('confidence_score', 0.0) < 0.7,
        'baseline_available': baseline_path.exists(),
        'baseline_confidence': baseline_result.get('confidence_score', 0),
        'confidence_improvement': enhanced_result.get('confidence_score', 0) - baseline_result.get('confidence_score', 0)
    }
    
    return enhanced_result

# FOCUSED FIX for enhanced_run_all_robust.py 
# Add this at the beginning of your enhanced_run_all_robust.py file, right after the imports

# Add this debugging function to see exactly what's happening
def debug_file_processing():
    """Debug function to see what files are being found and processed"""
    
    mock_dir = Path(__file__).resolve().parent / "data" / "mock_files"
    print(f"üîç DEBUG: Checking directory {mock_dir}")
    print(f"üîç DEBUG: Directory exists: {mock_dir.exists()}")
    
    if mock_dir.exists():
        all_files = list(mock_dir.iterdir())
        print(f"üîç DEBUG: Total files in directory: {len(all_files)}")
        
        for file_path in all_files:
            print(f"üîç DEBUG: Found {file_path.name}")
            print(f"   - Is file: {file_path.is_file()}")
            print(f"   - Extension: '{file_path.suffix.lower()}'")
            print(f"   - Size: {file_path.stat().st_size if file_path.is_file() else 'N/A'}")
            
            if file_path.suffix.lower() == '.txt':
                print(f"   üéØ This is a .txt file!")
                try:
                    content = file_path.read_text(encoding='utf-8')[:200]
                    print(f"   üìÑ Content preview: {repr(content[:100])}")
                except Exception as e:
                    print(f"   ‚ùå Cannot read: {e}")

# REPLACE the main() function with this version that has more explicit debugging:

def main():
    """Main robust processing pipeline with comprehensive debugging"""
    print("üõ†Ô∏è Starting ROBUST Enhanced Document Processing Pipeline")
    print("üéØ Features: LLM Enhancement + Baseline Comparison + Learning System")
    print("=" * 80)
    
    # Add debug output
    debug_file_processing()
    
    if not mock_dir.exists():
        print(f"‚ùå Mock files directory not found: {mock_dir}")
        print("üí° Run 'python config.py create-mocks' to create sample files")
        return
    
    # EXPLICIT file finding with detailed logging
    all_files = list(mock_dir.iterdir())
    print(f"\nüìÇ Directory scan results:")
    print(f"   Total items found: {len(all_files)}")
    
    files_to_process = []
    skipped_files = []
    
    # Check each file explicitly
    for file_path in sorted(all_files):
        print(f"\nüîé Examining: {file_path.name}")
        
        # Skip hidden files
        if file_path.name.startswith('.') or file_path.name.lower() == '.gitkeep':
            print(f"   ‚è≠Ô∏è SKIP: Hidden/system file")
            continue
            
        # Skip non-files
        if not file_path.is_file():
            print(f"   ‚è≠Ô∏è SKIP: Not a file (directory/symlink)")
            continue
        
        ext = file_path.suffix.lower()
        file_size = file_path.stat().st_size
        
        print(f"   üìã Extension: '{ext}' (empty if no extension)")
        print(f"   üìã Size: {file_size} bytes")
        
        # EXPLICIT checks for .txt files
        if ext == '.txt':
            print(f"   üéØ EXPLICIT .txt MATCH - WILL PROCESS")
            files_to_process.append(file_path)
            continue
        
        # Check other known extensions
        if ext in ['.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.xlsx', '.xls', '.csv']:
            print(f"   ‚úÖ STANDARD FORMAT - WILL PROCESS")
            files_to_process.append(file_path)
            continue
            
        # Check document formats
        if ext in ['.doc', '.docx', '.rtf', '.md', '.log', '.text']:
            print(f"   üìù DOCUMENT FORMAT - WILL PROCESS")
            files_to_process.append(file_path)
            continue
        
        # Check text content for unknown formats
        print(f"   ü§î UNKNOWN FORMAT - checking text content...")
        if file_size < 1024 * 1024:  # Less than 1MB
            try:
                content_sample = file_path.read_text(encoding='utf-8')[:100]
                if content_sample.strip() and len(content_sample.strip()) > 10:
                    print(f"   ‚úÖ HAS TEXT CONTENT - WILL PROCESS")
                    files_to_process.append(file_path)
                    continue
                else:
                    print(f"   ‚ùå NO MEANINGFUL TEXT")
            except Exception as e:
                print(f"   ‚ùå CANNOT READ AS TEXT: {e}")
        else:
            print(f"   ‚ùå TOO LARGE for unknown format ({file_size} bytes)")
        
        print(f"   ‚è≠Ô∏è SKIP: Cannot process")
        skipped_files.append(file_path)
    
    # Results summary
    print(f"\nüìä FILE PROCESSING SUMMARY:")
    print(f"   üìÅ Files to process: {len(files_to_process)}")
    print(f"   ‚è≠Ô∏è Files to skip: {len(skipped_files)}")
    
    # Show what will be processed
    if files_to_process:
        print(f"\n‚úÖ WILL PROCESS THESE FILES:")
        for f in files_to_process:
            print(f"   - {f.name} ({f.suffix if f.suffix else 'no extension'})")
    
    if skipped_files:
        print(f"\n‚ùå WILL SKIP THESE FILES:")
        for f in skipped_files:
            print(f"   - {f.name} ({f.suffix if f.suffix else 'no extension'})")
    
    if not files_to_process:
        print("\n‚ùå NO FILES TO PROCESS!")
        print("üí° Check if your .txt files exist and are readable")
        return
    
    print(f"\nü§ñ LLM Enhancement: {'‚úÖ AVAILABLE' if llm_enhancer else '‚ùå NOT AVAILABLE'}")
    
    # Process each file
    processed_files = []
    successful_enhancements = 0
    
    for i, file_path in enumerate(files_to_process, 1):
        print(f"\n{'='*80}")
        print(f"[{i}/{len(files_to_process)}] PROCESSING: {file_path.name}")
        print(f"{'='*80}")
        
        try:
            enhanced_result = process_file_robust(file_path)
            
            # Save result
            enhanced_filename = get_enhanced_filename(file_path)
            enhanced_path = enhanced_out_dir / enhanced_filename
            
            with open(enhanced_path, 'w', encoding='utf-8') as f:
                json.dump(enhanced_result, f, indent=2, default=str)
            
            print(f"  üíæ SAVED: {enhanced_path.name}")
            
            if enhanced_result.get('enhancement_info', {}).get('enhanced_by_llm'):
                successful_enhancements += 1
            
            processed_files.append(enhanced_result)
            
        except Exception as e:
            print(f"‚ùå PROCESSING FAILED for {file_path.name}: {e}")
            import traceback
            print(f"   Full traceback: {traceback.format_exc()}")
    
    # Final summary
    print(f"\n{'='*80}")
    print(f"üéâ PROCESSING COMPLETE!")
    print(f"{'='*80}")
    print(f"   Total processed: {len(processed_files)}")
    print(f"   LLM enhanced: {successful_enhancements}")
    print(f"   Output directory: {enhanced_out_dir}")
    
    if processed_files:
        avg_confidence = sum(f.get('confidence_score', 0) for f in processed_files) / len(processed_files)
        print(f"   Average confidence: {avg_confidence:.2f}")
    
    print(f"\nüí° Next step: streamlit run src/ui/enhanced_reviewer.py")

# ALSO add this simpler txt handler if the original one is causing issues:
def extract_from_txt_simple(path):
    """Simplified .txt extraction that always works"""
    try:
        content = Path(path).read_text(encoding='utf-8')
        print(f"   üìÑ Read {len(content)} characters from .txt file")
        
        result = {
            'order_id': 'TXT-EXTRACTED',  # Placeholder to show it worked
            'client_name': 'Text File Client',
            'order_date': None,
            'delivery_date': None,
            'items': [{'description': 'Text file content', 'raw_content': content[:200]}],
            'order_total': None,
            'currency': None,
            'special_instructions': None,
            'confidence_score': 0.5,  # Medium confidence to trigger LLM enhancement
            'file_type': 'txt',
            'processing_note': 'Basic .txt extraction successful'
        }
        
        return result
        
    except Exception as e:
        print(f"   ‚ùå .txt extraction failed: {e}")
        return {'error': f'txt extraction failed: {e}', 'confidence_score': 0.0}

# UPDATE the handlers dictionary to use the simple version:
handlers['.txt'] = lambda p: extract_from_txt_simple(str(p))
handlers['.text'] = lambda p: extract_from_txt_simple(str(p))

print("üîß FOCUSED FIX LOADED - .txt files should now be processed!")




if __name__ == '__main__':
    main()